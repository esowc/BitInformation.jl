var documenterSearchIndex = {"docs":
[{"location":"functions/#Index-of-functions-in-BitInformation.jl","page":"Function index","title":"Index of functions in BitInformation.jl","text":"","category":"section"},{"location":"functions/#Significance-of-information","page":"Function index","title":"Significance of information","text":"","category":"section"},{"location":"functions/","page":"Function index","title":"Function index","text":"BitInformation.binom_confidence(::Int,::Real)\nBitInformation.binom_free_entropy(::Int,::Real)","category":"page"},{"location":"functions/#BitInformation.binom_confidence-Tuple{Int64, Real}","page":"Function index","title":"BitInformation.binom_confidence","text":"p₁ = binom_confidence(n::Int,c::Real)\n\nReturns the probability p₁ of successes in the binomial distribution (p=1/2) of n trials with confidence c.\n\nExample\n\nAt c=0.95, i.e. 95% confidence, n=1000 tosses of  a coin will yield not more than\n\njulia> p₁ = BitInformation.binom_confidence(1000,0.95)\n0.5309897516152281\n\nabout 53.1% heads (or tails).\n\n\n\n\n\n","category":"method"},{"location":"functions/#BitInformation.binom_free_entropy-Tuple{Int64, Real}","page":"Function index","title":"BitInformation.binom_free_entropy","text":"Hf = binom_free_entropy(n::Int,c::Real,base::Real=2)\n\nReturns the free entropy Hf associated with binom_confidence.\n\n\n\n\n\n","category":"method"},{"location":"functions/#Transformations","page":"Function index","title":"Transformations","text":"","category":"section"},{"location":"functions/","page":"Function index","title":"Function index","text":"bittranspose(::AbstractArray)\nbitbacktranspose(::AbstractArray)\nxor_delta(::Array{AbstractFloat,1})\nunxor_delta(::Array{AbstractFloat,1})\nsigned_exponent(::Array{Float32})\nsigned_exponent!(::Array{Float32})","category":"page"},{"location":"functions/#BitInformation.bittranspose-Tuple{AbstractArray}","page":"Function index","title":"BitInformation.bittranspose","text":"Transpose the bits (aka bit shuffle) of an array to place sign bits, etc. next to each other in memory. Back transpose via bitbacktranspose().\n\n\n\n\n\n","category":"method"},{"location":"functions/#BitInformation.bitbacktranspose-Tuple{AbstractArray}","page":"Function index","title":"BitInformation.bitbacktranspose","text":"Backtranspose the bits of array A that were previously transposed with bittranspse().\n\n\n\n\n\n","category":"method"},{"location":"functions/#BitInformation.xor_delta-Tuple{Vector{AbstractFloat}}","page":"Function index","title":"BitInformation.xor_delta","text":"Bitwise XOR delta. Elements include A are XORed with the previous one. The first element is left unchanged. E.g. [0b0011,0b0010] -> [0b0011,0b0001]. \n\n\n\n\n\n","category":"method"},{"location":"functions/#BitInformation.unxor_delta-Tuple{Vector{AbstractFloat}}","page":"Function index","title":"BitInformation.unxor_delta","text":"Undo bitwise XOR delta. Elements include A are XORed again to reverse xor_delta. E.g. [0b0011,0b0001] -> [0b0011,0b0010] \n\n\n\n\n\n","category":"method"},{"location":"functions/#BitInformation.signed_exponent-Tuple{Array{Float32, N} where N}","page":"Function index","title":"BitInformation.signed_exponent","text":"B = signed_exponent(A::Array{T}) where {T<:Union{Float16,Float32,Float64}}\n\nConverts the exponent bits of Float16,Float32 or Float64-arrays from its conventional biased-form into a sign&magnitude representation.\n\nExample\n\njulia> bitstring(10f0,:split)\n\"0 10000010 01000000000000000000000\"\n\njulia> bitstring.(signed_exponent([10f0]),:split)[1]\n\"0 00000011 01000000000000000000000\"\n\nIn the former the exponent 3 is interpret from 0b10000010=130 via subtraction of the exponent bias of Float32 = 127. In the latter the exponent is inferred from sign bit (0) and a magnitude represetation 2^1 + 2^1 = 3.\n\n\n\n\n\n","category":"method"},{"location":"rounding/#Rounding","page":"Rounding","title":"Rounding","text":"","category":"section"},{"location":"rounding/","page":"Rounding","title":"Rounding","text":"Rounding generally replaces a value x with an approximation hatx, which is from a smaller set of  representable values (e.g. with fewer decimal or binary places of accuracy). Binary rounding removes the  information in the n last bits by setting them to 0 (or 1). Several rounding modes exist, and  BitInformation.jl implements them efficiently with bitwise operations, in-place or by creating a  copy of the original array. ","category":"page"},{"location":"rounding/#Round-to-nearest","page":"Rounding","title":"Round to nearest","text":"","category":"section"},{"location":"rounding/","page":"Rounding","title":"Rounding","text":"With binary round to nearest a full-precision number is replaced by the nearest representable float with fewer mantissa bits by rounding the trailing bits to zero. BitInformation.jl implements this by extending Julia's round to round(::Array{T},n::Integer) where T either Float32 or Float64 and n the number of significant bits retained after rounding. Negative n are possible too, which will round even the exponent bits.","category":"page"},{"location":"rounding/","page":"Rounding","title":"Rounding","text":"Rounding","category":"page"},{"location":"rounding/","page":"Rounding","title":"Rounding","text":"julia> # bitwise representation (split in sign, exp, sig bits) of some random numbers\njulia> bitstring.(A,:split)             \n5-element Array{String,1}:\n \"0 01111101 01001000111110101001000\"\n \"0 01111110 01010000000101001110110\"\n \"0 01111110 01011101110110001000110\"\n \"0 01111101 00010101010111011100000\"\n \"0 01111001 11110000000000000000101\"","category":"page"},{"location":"rounding/","page":"Rounding","title":"Rounding","text":"to n=3 significant bits via round(A,3) yields","category":"page"},{"location":"rounding/","page":"Rounding","title":"Rounding","text":"julia> bitstring.(round(A,3),:split)\n5-element Array{String,1}:\n \"0 01111101 01000000000000000000000\"\n \"0 01111110 01100000000000000000000\"  # round up, flipping the third significant bit\n \"0 01111110 01100000000000000000000\"  # same here\n \"0 01111101 00100000000000000000000\"  # and here\n \"0 01111010 00000000000000000000000\"  # note how the carry bits correctly carries into the exponent","category":"page"},{"location":"rounding/","page":"Rounding","title":"Rounding","text":"This rounding function is IEEE compatible as it also implements tie-to-even, meaning that 01 which is exactly halway between 0 and 1 is round to 0 which is the even number (a bit sequence ending in a 0 is even). Similarly, 11 is round up to 100 and not down to 10. Rounding to 1 signficant bit means that only 1,1.5,2,3,4,6... are representable.","category":"page"},{"location":"rounding/","page":"Rounding","title":"Rounding","text":"julia> A = Float32[1.25,1.5,1.75]\njulia> bitstring.(A,:split)\n3-element Vector{String}:\n \"0 01111111 01000000000000000000000\"  \n \"0 01111111 10000000000000000000000\"\n \"0 01111111 11000000000000000000000\"\n\njulia> bitstring.(round(A,1),:split)\n3-element Vector{String}:\n \"0 01111111 00000000000000000000000\"  # 1.25 is tie between 1.0 and 1.5, round down to even\n \"0 01111111 10000000000000000000000\"  # 1.5 is representable, no rounding\n \"0 10000000 00000000000000000000000\"  # 1.75 is tie between 1.5 and 2.0, round up to even","category":"page"},{"location":"rounding/#Bit-shave","page":"Rounding","title":"Bit shave","text":"","category":"section"},{"location":"rounding/","page":"Rounding","title":"Rounding","text":"In contrast to round to nearest, shave will always round to zero by shaving the trailing significant bits off (i.e. set them to zero). This rounding mode therefore introduces a bias towards 0 and the rounding error can be twice as large as for round to nearest.","category":"page"},{"location":"rounding/","page":"Rounding","title":"Rounding","text":"julia> bitstring.(shave(A,3),:split)\n5-element Array{String,1}:\n \"0 01111101 01000000000000000000000\"  # identical to round here\n \"0 01111110 01000000000000000000000\"  # round down here, whereas `round` would round up\n \"0 01111110 01000000000000000000000\"\n \"0 01111101 00000000000000000000000\"\n \"0 01111001 11100000000000000000000\"  # no carry bit for `shave`","category":"page"},{"location":"rounding/#Bit-set","page":"Rounding","title":"Bit set","text":"","category":"section"},{"location":"rounding/","page":"Rounding","title":"Rounding","text":"Similar to shave, set_one will always set the trailing significant bits to 1. This rounding mode therefore introduces a bias away from 0 and the rounding error can be twice as large as for round to nearest.","category":"page"},{"location":"rounding/","page":"Rounding","title":"Rounding","text":"julia> bitstring.(set_one(A,3),:split)\n5-element Array{String,1}:\n \"0 01111101 01011111111111111111111\"  # all trailing bits are always 1\n \"0 01111110 01011111111111111111111\"\n \"0 01111110 01011111111111111111111\"\n \"0 01111101 00011111111111111111111\"\n \"0 01111001 11111111111111111111111\"","category":"page"},{"location":"rounding/#Bit-groom","page":"Rounding","title":"Bit groom","text":"","category":"section"},{"location":"rounding/","page":"Rounding","title":"Rounding","text":"Combining shave and set_one, by alternating both removes the bias from both. This method is called grooming and is implemented via the groom function","category":"page"},{"location":"rounding/","page":"Rounding","title":"Rounding","text":"julia> bitstring.(groom(A,3),:split)\n5-element Array{String,1}:\n \"0 01111101 01000000000000000000000\"   # shave\n \"0 01111110 01011111111111111111111\"   # set to one\n \"0 01111110 01000000000000000000000\"   # shave\n \"0 01111101 00011111111111111111111\"   # etc.\n \"0 01111001 11100000000000000000000\"","category":"page"},{"location":"rounding/#Bit-halfshave","page":"Rounding","title":"Bit halfshave","text":"","category":"section"},{"location":"rounding/","page":"Rounding","title":"Rounding","text":"Another way to remove the bias from shave is to replace the trailing significant bits with 100... which is equivalent to round to nearest, but uses representable values that are always halfway between. This also removes the bias of shave or set_one and yields on average a rounding error that is as large as from round to nearest","category":"page"},{"location":"rounding/","page":"Rounding","title":"Rounding","text":"julia> bitstring.(halfshave(A,3),:split)\n5-element Array{String,1}:\n \"0 01111101 01010000000000000000000\"   # set all discarded bits to 1000...\n \"0 01111110 01010000000000000000000\"\n \"0 01111110 01010000000000000000000\"\n \"0 01111101 00010000000000000000000\"\n \"0 01111001 11110000000000000000000\"","category":"page"},{"location":"bitinformation/#Bitwise-information-content-analysis","page":"Bitwise information","title":"Bitwise information content analysis","text":"","category":"section"},{"location":"bitinformation/#Bitpattern-entropy","page":"Bitwise information","title":"Bitpattern entropy","text":"","category":"section"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"An n-bit number format has  bitpatterns available to encode a real number.  For most data arrays, not all bitpatterns are used at uniform probability.  The bitpattern entropy is the  Shannon information entropy H, in units of bits, calculated from the probability p_i of each bitpattern ","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"H = -sum_i=1^2^np_i log_2(p_i)","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"The bitpattern entropy is H leq n and maximised to n bits for a uniform distribution. The free entropy is the difference n-H.","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"In BitInformation.jl, the bitpattern entropy is calculated via bitpattern_entropy(::Array)","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"julia> A = rand(Float32,100000000);\n\njulia> bitpattern_entropy(A)\n22.938590744784577","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"Here, the entropy is about 23 bit, meaning that 9 bits are effectively unused. This is because rand samples in [1,2), so that the sign and exponent bits are always 0 01111111 followed by some random significant bits.","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"The function bitpattern_entropy is based on sorting the array A. While this avoids the allocation of a bitpattern histogram (which would make the function unsuitable for anything larger than 32 bits) it has to allocate a sorted version of A.","category":"page"},{"location":"bitinformation/#Bit-count-entropy","page":"Bitwise information","title":"Bit count entropy","text":"","category":"section"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"The Shannon information entropy H, in unit of bits, takes for a bitstream b=b_1b_2b_kb_l, i.e. a sequence of bits of length l, the form","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"H(b) = -p_0 log_2(p_0) - p_1log_2(p_1)","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"with p_0p_1 being the probability of a bit b_k in b being 0 or 1. The entropy is maximised to 1 bit for equal probabilities p_0 = p_1 = tfrac12 in b. The function bitcount(A::Array) counts all occurences of the 1-bit in every bit-position in every element of A. E.g.","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"julia> bitstring.(A)        # 5-elemenet Vector{UInt8}\n5-element Array{String,1}:\n \"10001111\"\n \"00010111\"\n \"11101000\"\n \"10100100\"\n \"11101011\"\n\njulia> bitcount(A)\n8-element Array{Int64,1}:\n 4                          # number of 1-bits in the first bit of UInt8\n 2                          # in the second bit position\n 3                          # etc.\n 1\n 3\n 3\n 3\n 3","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"The first bit of elements (here: UInt8) in A are 4 times 1 and so 1 times 0, etc. In contrast, elements drawn from a uniform distribution U(0,1)","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"julia> A = rand(Float32,100000);\n\njulia> bitcount(A)\n32-element Array{Int64,1}:\n      0\n      0\n 100000\n 100000\n      ⋮\n  37411\n  25182\n      0","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"have never a sign bit that is 1, but the 2nd and third exponent bit is always 1. The last significant bits in rand do not occur at 50% chance, which is due to the pseudo-random number generator (see a discussion here).","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"Once the bits in an array are counted, the respective probabilities p_0p_1 can be calculated and the entropy derived. The function bitcount_entropy(A::Array) does that","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"julia> A = rand(UInt8,100000);          # entirely random bits\njulia> Elefridge.bitcountentropy(A)\n8-element Array{Float64,1}:             # entropy is for every bit position ≈ 1\n 0.9999998727542938\n 0.9999952725717266\n 0.9999949724904816\n 0.9999973408228667\n 0.9999937649515901\n 0.999992796900212\n 0.9999970566115759\n 0.9999998958374157","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"This converges to 1 for larger arrays.","category":"page"},{"location":"bitinformation/#Bit-pair-count","page":"Bitwise information","title":"Bit pair count","text":"","category":"section"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"The bitpaircount(A::Array) function returns a 4xn (with n being the number of bits in every element of A) array, the counts the occurrences of 00,01,10,11 for all bit-positions in a in A across all elements a in A. For a length N of array A (one or multi-dimensional) the maximum occurrence is N-1. E.g.","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"julia> A = rand(UInt8,5);\njulia> bitstring.(A)\n5-element Array{String,1}:\n \"01000010\"\n \"11110110\"\n \"01010110\"\n \"01111111\"\n \"00010100\"\n\njulia> bitpaircount(A)\n4×8 Array{Int64,2}:\n 2  0  0  0  2  0  0  2    # occurences of `00` in the n-bits of UInt8\n 1  0  2  1  1  1  0  1    # occurences of `01`\n 1  1  2  0  1  0  1  1    # occurences of `10`\n 0  3  0  3  0  3  3  0    # occurences of `11`","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"The first bit of elements in A is as a sequence 01000. Consequently, 00 occurs 2x, 01 and 10 once each, and 11 does not occur. Multi-dimensional arrays are unravelled into a vector, following Julia's memory layout (column-major).","category":"page"},{"location":"bitinformation/#Bit-conditional-entropy","page":"Bitwise information","title":"Bit conditional entropy","text":"","category":"section"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"Based on bitpaircount we can calculate the conditional entropy of the state of one bit given the state of the previous bit (previous meaning in the same bit position but in the previous element in the array A). In the previous example we obtain","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"julia> bit_condprobability(A)\n4×8 Array{Float64,2}:\n 0.666667  NaN     0.0  0.0  0.666667  0.0  NaN     0.666667\n 0.333333  NaN     1.0  1.0  0.333333  1.0  NaN     0.333333\n 1.0         0.25  1.0  0.0  1.0       0.0    0.25  1.0\n 0.0         0.75  0.0  1.0  0.0       1.0    0.75  0.0","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"Given the previous bit being 0 there is a 2/3 chance that th next bit is a 0 too, and a 1/3 change that the next bit is a 1, i.e. p_00 = p(textnext=0textprevious=0) = 23, and p_10 = p(10) = tfrac13, such that p(00)+p(10)=1 always (which are the marginal probabilities from below), if not NaN, and similarly for p(01) and p(11).","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"The conditional entropies H_0H_1 are conditioned on the state of the previous bit b_j-1 being 0 or 1","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"beginaligned\nH_0 = -p_00log_2(p_00) - p_01log_2(p_01) \nH_1 = -p_10log_2(p_10) - p_11log_2(p_11) \nendaligned","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"The conditional entropy is maximised to 1 bit for bitstreams where the probability of a bit being 0 or 1 does not depend on the state of the previous bit, which is here defined as false information.","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"julia> r = rand(Float32,100_000)\njulia> H₀,H₁ = bit_condentropy(r)\njulia> H₀\n32-element Vector{Float64}:\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.07559467763419292\n 0.5214998930042997\n 0.9684130809383832\n ⋮\n 0.9997866754890564\n 0.999747731180627\n 0.999438123786493\n 0.9968145441905049\n 0.9878425610244357\n 0.9528602299665989\n 0.8124289058679582\n 0.0","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"Sign and the first exponent bits have 0 conditional entropy, which increases to 1 bit for the fully random significant bits. The last significant bits have lower conditional entropy due to shortcomings in the pseudo random generator in rand, see a discussion here.","category":"page"},{"location":"bitinformation/#Mutual-information","page":"Bitwise information","title":"Mutual information","text":"","category":"section"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"The mutual information of two bitstreams (which can be, for example, two arrays, or adjacent  elements in one array) r = r_1r_2r_kr_l and s = s_1s_2s_ks_l is defined via the joint probability mass function p_rs which here takes the form of a 2x2 matrix","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"p_rs = beginpmatrixp_00  p_01  p_10  p_11 endpmatrix","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"with p_ij being the probability that the bits are in the state r_k=i and s_k = j simultaneously and p_00+p_01+p_10+p_11 = 1. The marginal probabilities follow as column or row-wise additions in p_rs, e.g. the probability that r_k = 0 is p_r=0 = p_00 + p_01. The mutual information M(rs) of the two bitstreams rs is then","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"M(rs) = sum_r=0^1 sum_s=0^1 p_rs log_2 left( fracp_rsp_r=rp_s=sright)","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"The function bitinformation(::Array{T,N},::Array{T,N}) calculates M as","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"julia> r = rand(Float32,100_000)    # [0,1) random float32s\njulia> s = shave(r,15)              # remove information in sbit 16-23 by setting to 0\njulia> bitinformation(r,s)\n32-element Vector{Float64}:\n 0.0\n ⋮\n 0.9999935941359982                 # sbit 12: 1 bit of mutual information\n 0.9999912641753561                 # sbit 13: same\n 0.9999995383375376                 # sbit 14: same\n 0.9999954191498579                 # sbit 15: same\n 0.0                                # sbit 16: always 0 in s, but random in r: M=0 bits\n 0.0                                # sbit 17: same\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0                                # sbit 23","category":"page"},{"location":"bitinformation/#Real-bitwise-information","page":"Bitwise information","title":"Real bitwise information","text":"","category":"section"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"The mutual information of bits from adjacent bits is the bitwise real information content and derived as follows. For the two bitstreams rs being the preceding and succeeding bits (for example in space or time) in a single bitstream b, i.e. r=b_1b_2b_l-1 and s=b_2b_3b_l the unconditional entropy is then effectively H = H(r) = H(s) for l being very large. We then can write the mutual information M(rs) between adjacent bits also as ","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"I = H - q_0H_0 - q_1H_1","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"which is the real information content I. This definition is similar to Jeffress et al. (2017) [1], but avoids an additional assumption of an uncertainty measure. This defines the real information as the entropy minus the false information.  For bitstreams with either p_0 = 1 or p_1 = 1, i.e. all bits are either 0 or 1, the entropies are zero H = H_0 = H_1 = 0 and we may refer to the bits in the bitstream as being unused. In the case where H  p_0H_0 + p_1H_1, the preceding bit is a predictor for the succeeding bit which means that the bitstream contains real information (I  0).","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"The computation of I is implemented in bitinformation(::Array) ","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"julia> A = rand(UInt8,1000000)  # fully random bits\njulia> bitinformation(A)\n8-element Array{Float64,1}:\n 0.0                            # real information = 0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"The information of random uniform bits is 0 as the knowledge of a given bit does not provide any information for the succeeding bits. However, correlated arrays (which we achieve here by sorting)","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"julia> A = rand(Float32,1000000)\njulia> sort!(A)\njulia> bitinformation(A)\n32-element Vector{Float64}:\n 0.0\n 0.0\n 0.0\n 0.0\n 0.00046647589813905157\n 0.06406479945998214\n 0.5158447841492068\n 0.9704486460488391\n 0.9150881582169795\n 0.996120575536068\n 0.9931335810218149\n ⋮\n 0.15992667263039423\n 0.0460430997651915\n 0.006067325343418917\n 0.0008767479258913191\n 0.00033132201520535975\n 0.0007048623462190817\n 0.0025481588434255187\n 0.0087191715755926\n 0.028826838913308506\n 0.07469492765760763\n 0.0","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"have only zero information in the sign (unused for random uniform distribution U(0,1)), and in the first exponent bits (also unused due to limited range) and in the last significant bit (flips randomly). The information is maximised to 1 bit for the last exponent and the first significant bits, as knowing the state of such a bit one can expect the next (or previous) bit to be the same due to the correlation.","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"[1] Jeffress, S., Düben, P. & Palmer, T. Bitwise efficiency in chaotic models. Proc. R. Soc. Math. Phys. Eng. Sci. 473, 20170144 (2017).","category":"page"},{"location":"bitinformation/#Multi-dimensional-real-information","page":"Bitwise information","title":"Multi-dimensional real information","text":"","category":"section"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"The real information content I_m  for an m-dimensional array A is the sum of the real information along the  dimensions. Let b_j be a bitstream obtained by unravelling a given bitposition in  along its j-th dimension. Although the unconditional entropy H is unchanged along the m-dimensions, the conditional entropies H_0H_1 change as the preceding and succeeding bit is found in another dimension, e.g. b_2 is obtained by re-ordering b_1. Normalization by tfrac1m is applied to I_m such that the maximum information is 1 bit in I_m^*","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"I_m^* = H - fracp_0msum_j=1^mH_0(b_j) - fracp_1msum_j=1^mH_1(b_j)","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"This is implemented in BitInformation.jl as bitinformation(::Array{T,N},:all_dimensions), e.g.","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"julia> A = rand(Float32,100,200,300)    # a 3D array\njulia> sort!(A,dims=1)                  # sort to create some auto-corelation\njulia> bitinformation(A,:all_dimensions)\n32-element Vector{Float64}:\n 0.0\n 0.0\n 0.0\n 0.0\n 6.447635701154324e-7\n 0.014292670110681693\n 0.31991625275425073\n 0.5440816091704278\n 0.36657938793446365\n 0.2533186226597825\n 0.13051374121057438\n ⋮\n 0.0\n 0.0\n 8.687738656254496e-7\n 6.251449893598012e-6\n 5.0038146715651134e-5\n 0.0003054976017733783\n 0.0015377166906772044\n 0.006581160530812665\n 0.022911924843179426\n 0.06155167545633838\n 0.0","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"which is equivalent to","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"julia> bitinformation(A,:all_dimensions) ≈ 1/3*(bitinformation(A,dims=1)+\n                                               bitinformation(A,dims=2)+\n                                               bitinformation(A,dims=3))\ntrue","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"The keyword dims will permute the dimensions in A to calcualte the information in the specified dimensions. By default dims=1, which uses the ordering of the bits as they are layed out in memory.","category":"page"},{"location":"bitinformation/#Redundancy","page":"Bitwise information","title":"Redundancy","text":"","category":"section"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"Redundancy R is defined as the symmetric normalised mutual information M(rs)","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"R(rs) = frac2M(rs)H(r) + H(s)","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"R is the redundancy of information of r in s (and vice versa). R = 1 for identical bitstreams r = s, but R = 0 for statistically independent bitstreams.","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"BitInformation.jl implements the redundancy calculation via redundancy(::Array{T,N},::Array{T,N}) where the inputs have to be of same size and element type T. For example, shaving off some of the last significant bits will set the redundancy for those to 0, but redundancy is 1 for all bitstreams which are identical","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"julia> r = rand(Float32,100_000)    # random data\njulia> s = shave(r,7)               # keep only sign, exp and sbits 1-7\njulia> redundancy(r,s)\n32-element Vector{Float64}:\n 0.0                                # 0 redundancy as entropy = 0\n 0.0\n 0.0\n 0.0\n 0.9999999999993566                 # redundancy 1 as bitstreams are identical\n 0.9999999999999962\n 1.0\n 1.0000000000000002\n  ⋮\n 0.0                                # redundancy 0 as information lost in shave\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0","category":"page"},{"location":"bitinformation/#Preserved-information","page":"Bitwise information","title":"Preserved information","text":"","category":"section"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"The preserved information P(rs) between two bitstreams rs where s approximates r is the redundancy-weighted real information I","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"P(rs) = R(rs)I(r)","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"The information loss L is 1-P and represents the unpreserved information of r in s. In most cases we are interested in the preserved information of an array X = (x_1x_2x_qx_n) of bitstreams  when approximated by a previously compressed array Y = (y_1y_2y_qy_n). For an array A of floats with n=32 bit, for example, x_1 is the bitstream of all sign bits unravelled along a given dimension (eg longitudes) and x_32 is the bitstream of the last mantissa bits. The redundancy R(XY) and the real information I(X) is then calculated for each bit position q individually, and the preserved information P is the redundancy-weighted mean of the real information in X","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"P(XY) = fracsum_q=1^n R(x_qy_q)I(x_q)sum_q=1^n I(x_q)","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"The quantity sum_q=1^n I(x_q) is the total information in X and therefore also in A.  The redundancy is R=1 for bits that are unchanged during rounding and R=0 for bits that are round to zero. Example","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"julia> r = rand(Float32,100_000)    # random bits\njulia> sort!(r)                     # sort to introduce auto-correlation & statistical dependence of bits\njulia> s = shave(r,7)               # s is an approximation to r, shaving off sbits 8-23\njulia> R = redundancy(r,s)          \njulia> I = bitinformation(r)\njulia> P = (R'*I)/sum(I)            # preserved information of r in s\n0.9087255894613658                  # = 91%","category":"page"},{"location":"bitinformation/#Significance-of-information","page":"Bitwise information","title":"Significance of information","text":"","category":"section"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"For an entirely independent and approximately equal occurrence of bits in a bitstream of length l, the probabilities p_0p_1 of a bit being 0 or 1 approach p_0approxp_1approxtfrac12, but they are in practice not equal for l  infty. Consequently, the entropy is smaller than 1, but only insignificantly. The probability p_1 of successes in the binomial distribution (with parameter p=tfrac12) with l trials (using the normal approximation for large l) is","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"p_1 = frac12 + fracz2sqrtl","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"where z is the 1-tfrac12(1-c) quantile at confidence level c of the standard normal distribution. For c=099, corresponding to a 99%-confidence level which is used as default here, z=258 and for l=10^7 a probability tfrac12 leq p leq p_1 = 05004 is considered insignificantly different from equal occurrence p_0 = p_1. This is implemented as binom_confidence(l,c)","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"julia> BitInformation.binom_confidence(10_000_000,0.99)\n0.500407274373151","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"The associated free entropy H_f in units of bits follows as","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"H_f = 1 - p_1log_2(p_1) - (1-p_1)log_2(1-p_1)","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"And we consider real information below H_f as insignificantly different from 0 and set the real information I = 0. The calculation of H_f is implemented as binom_free_entropy(l,c,base=2)","category":"page"},{"location":"bitinformation/","page":"Bitwise information","title":"Bitwise information","text":"julia> BitInformation.binom_free_entropy(10_000_000,0.99)\n4.786066739592698e-7","category":"page"},{"location":"#BitInformation.jl-documentation","page":"Home","title":"BitInformation.jl documentation","text":"","category":"section"},{"location":"#Overview","page":"Home","title":"Overview","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"BitInformation.jl is a library for the analysis of bitwise information in n-dimensional Julia arrays. ","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"BitInformation.jl is registered in the Julia Registry, so just do","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia>] add BitInformation","category":"page"},{"location":"","page":"Home","title":"Home","text":"where ] opens the package manager. The latest version is automatically installed.","category":"page"},{"location":"#Developers","page":"Home","title":"Developers","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"BitInformation.jl is currently developed by Milan Klöwer, any contributions are always welcome.","category":"page"},{"location":"#Funding","page":"Home","title":"Funding","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This project is funded by the Copernicus Programme through the ECMWF summer of weather code 2020 and 2021.","category":"page"},{"location":"transformations/#Bit-transformations","page":"Transformations","title":"Bit transformations","text":"","category":"section"},{"location":"transformations/","page":"Transformations","title":"Transformations","text":"BitInformation.jl implements several bit transformations, meaning reversible, bitwise operations on scalars or arrays that reorder or transform the bits. This is often used to pre-process the data to make it more suitable for lossless compression algorithms.","category":"page"},{"location":"transformations/","page":"Transformations","title":"Transformations","text":"warning: Interpretation of transformed floats\nBitInformation.jl will not store the information that a transformation was applied to a value. This means that Julia will not know about this and interpret a value incorrectly. You will have to explicitly execute the backtransform julia> A = [0f0,1f0]         # 0 and 1\njulia> At = bittranspose(A)  # are transposed into 1f-35 and 0\n2-element Vector{Float32}:\n1.0026967f-35\n0.0","category":"page"},{"location":"transformations/#Bit-transpose-(aka-shuffle)","page":"Transformations","title":"Bit transpose (aka shuffle)","text":"","category":"section"},{"location":"transformations/","page":"Transformations","title":"Transformations","text":"Bit shuffle operations re-order the bits or bytes in an array, such that bits or each element in that array are placed next to each other in memory. Despite the name, this operation is often called \"shuffle\", although there is nothing random about this, and it is perfectly reversible. Here, we call it bit transpose, as for an array with n elements of each n bits, this is equivalent to the matrix tranpose","category":"page"},{"location":"transformations/","page":"Transformations","title":"Transformations","text":"julia> A = rand(UInt8,8);\njulia> bitstring.(A)\n8-element Array{String,1}:\n \"10101011\"\n \"11100000\"\n \"11010110\"\n \"10001101\"\n \"10000010\"\n \"00011110\"\n \"11111100\"\n \"00011011\"\n\njulia> At = bittranspose(A);\njulia> bitstring.(At)\n8-element Array{String,1}:\n \"11111010\"\n \"01100010\"\n \"11000010\"\n \"00100111\"\n \"10010111\"\n \"00110110\"\n \"10101101\"\n \"10010001\"","category":"page"},{"location":"transformations/","page":"Transformations","title":"Transformations","text":"In general, we can bittranspose n-element arrays with m bits bits per element, which corresponds to a reshaped transpose. For floats, bittranspose will place all the sign bits next to each other in memory, then all the first exponent bits and so on. Often this creates a better compressible array, as bits with similar meaning (and often the same state in correlated data) are placed next to each other.","category":"page"},{"location":"transformations/","page":"Transformations","title":"Transformations","text":"julia> A = rand(Float32,10);\njulia> Ar = round(A,7);\n\njulia> bitstring.(bittranspose(Ar))\n10-element Array{String,1}:\n \"00000000000000000000111111111111\"\n \"11111111111111111111111111111011\"\n \"11111101100001011100111010000001\"\n \"00111000001010001010100111101001\"\n \"00000101011101110110000101100010\"\n \"00000000000000000000000000000000\"\n \"00000000000000000000000000000000\"\n \"00000000000000000000000000000000\"\n \"00000000000000000000000000000000\"\n \"00000000000000000000000000000000\"","category":"page"},{"location":"transformations/","page":"Transformations","title":"Transformations","text":"Now all the sign bits are in the first row, and so on. Using round means that all the zeros from rounding are now placed at the end of the array. The bittranspose function can be reversed by bitbacktranspose:","category":"page"},{"location":"transformations/","page":"Transformations","title":"Transformations","text":"julia> A = rand(Float32,123,234);\n\njulia> A == bitbacktranspose(bittranspose(A))\ntrue","category":"page"},{"location":"transformations/","page":"Transformations","title":"Transformations","text":"Both accept arrays of any shape for UInts as well as floats.","category":"page"},{"location":"transformations/#XOR-delta","page":"Transformations","title":"XOR delta","text":"","category":"section"},{"location":"transformations/","page":"Transformations","title":"Transformations","text":"Instead of storing every element in an array as itself, you may want to store the difference to the previous value. For bits this \"difference\" generalises to the reversible xor-operation. The xor_delta function applies this operation to a UInt or Float array:","category":"page"},{"location":"transformations/","page":"Transformations","title":"Transformations","text":"julia> A = rand(UInt16,4)\n4-element Array{UInt16,1}:\n 0x2569\n 0x97d2\n 0x7274\n 0x4783\n\njulia> xor_delta(A)\n4-element Array{UInt16,1}:\n 0x2569\n 0xb2bb\n 0xe5a6\n 0x35f7","category":"page"},{"location":"transformations/","page":"Transformations","title":"Transformations","text":"And is reversible with unxor_delta.","category":"page"},{"location":"transformations/","page":"Transformations","title":"Transformations","text":"julia> A == unxor_delta(xor_delta(A))\ntrue","category":"page"},{"location":"transformations/","page":"Transformations","title":"Transformations","text":"This method is interesting for correlated data, as many bits will be 0 in the XORed array:","category":"page"},{"location":"transformations/","page":"Transformations","title":"Transformations","text":"julia> A = sort(1 .+ rand(Float32,100000));\njulia> Ax = xor_delta(A);\njulia> bitstring.(Ax)\n100000-element Array{String,1}:\n \"00111111100000000000000000000101\"\n \"00000000000000000000000010110011\"\n \"00000000000000000000000000001000\"\n \"00000000000000000000000001101110\"\n \"00000000000000000000000101101001\"\n \"00000000000000000000000001101100\"\n \"00000000000000000000001111011000\"\n \"00000000000000000000000010001101\"\n ⋮","category":"page"},{"location":"transformations/#Signed-exponent","page":"Transformations","title":"Signed exponent","text":"","category":"section"},{"location":"transformations/","page":"Transformations","title":"Transformations","text":"Floating-point numbers have a biased exponent. There are  other ways to encode the exponent and BitInformation.jl implements signed_exponent which transforms the exponent bits of a float into a  representation where also the exponent has a sign bit (which is the first exponent bit)","category":"page"},{"location":"transformations/","page":"Transformations","title":"Transformations","text":"julia> a = [0.5f0,1.5f0]               # smaller than 1 (exp sign -1), larger than 1 (exp sign +1)\njulia> bitstring.(a,:split)\n2-element Vector{String}:\n \"0 01111110 00000000000000000000000\"  # biased exponent: 2^(e-bias) = 2^-1 here\n \"0 01111111 10000000000000000000000\"  # biased exponent: 2^(e-bias) = 2^0 here\n\njulia> bitstring.(signed_exponent(a),:split)\n2-element Vector{String}:\n \"0 10000001 00000000000000000000000\"  # signed exponent: sign=1, magnitude=1, i.e. 2^-1\n \"0 00000000 10000000000000000000000\"  # signed exponent: sign=0, magnitude=0, i.e. 2^0","category":"page"}]
}
